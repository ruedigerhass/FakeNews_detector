{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are fake news?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libary import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2990,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2991,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "RSEED = 42\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import percentile\n",
    "\n",
    "#preprocessing\n",
    "import nltk \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#EDA\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#model\n",
    "from sklearn import model_selection\n",
    "\n",
    "## for bag-of-words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import feature_extraction, preprocessing, model_selection\n",
    "from sklearn import feature_selection, metrics, naive_bayes, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# word2vector\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from sklearn import manifold\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import pydot\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## bert language model\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2992,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostly true                  1264\n",
       "mixture of true and false     212\n",
       "mostly false                   87\n",
       "no factual content             64\n",
       "Name: veracity, dtype: int64"
      ]
     },
     "execution_count": 2992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_overview = pd.read_csv('overview.csv')\n",
    "data_overview.veracity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2993,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            XML        portal orientation                   veracity  \\\n",
       "0     0000.xml           abc  mainstream                mostly true   \n",
       "1     0001.xml           abc  mainstream                mostly true   \n",
       "2     0002.xml           abc  mainstream                mostly true   \n",
       "3     0003.xml           abc  mainstream                mostly true   \n",
       "4     0004.xml           abc  mainstream                mostly true   \n",
       "5     0005.xml           abc  mainstream                mostly true   \n",
       "6     0006.xml           abc  mainstream                mostly true   \n",
       "7     0007.xml           abc  mainstream         no factual content   \n",
       "8     0008.xml           abc  mainstream                mostly true   \n",
       "9     0009.xml           abc  mainstream                mostly true   \n",
       "10    0010.xml           abc  mainstream                mostly true   \n",
       "11    0011.xml           abc  mainstream                mostly true   \n",
       "12    0012.xml           abc  mainstream                mostly true   \n",
       "13    0013.xml           abc  mainstream                mostly true   \n",
       "14    0014.xml           abc  mainstream                mostly true   \n",
       "15    0015.xml           abc  mainstream                mostly true   \n",
       "16    0016.xml           abc  mainstream                mostly true   \n",
       "17    0017.xml           abc  mainstream                mostly true   \n",
       "18    0018.xml           abc  mainstream                mostly true   \n",
       "19    0019.xml           abc  mainstream  mixture of true and false   \n",
       "20    0020.xml           abc  mainstream                mostly true   \n",
       "21    0021.xml           abc  mainstream                mostly true   \n",
       "22    0022.xml           abc  mainstream                mostly true   \n",
       "23    0023.xml           abc  mainstream                mostly true   \n",
       "24    0024.xml           abc  mainstream                mostly true   \n",
       "25    0025.xml           abc  mainstream                mostly true   \n",
       "26    0026.xml           abc  mainstream                mostly true   \n",
       "27    0027.xml           abc  mainstream                mostly true   \n",
       "28    0028.xml           abc  mainstream                mostly true   \n",
       "29    0029.xml           abc  mainstream                mostly true   \n",
       "...        ...           ...         ...                        ...   \n",
       "1597  1597.xml  the-other-98        left                mostly true   \n",
       "1598  1598.xml  the-other-98        left                mostly true   \n",
       "1599  1599.xml  the-other-98        left                mostly true   \n",
       "1600  1600.xml  the-other-98        left                mostly true   \n",
       "1601  1601.xml  the-other-98        left                mostly true   \n",
       "1602  1602.xml  the-other-98        left                mostly true   \n",
       "1603  1603.xml  the-other-98        left                mostly true   \n",
       "1604  1604.xml  the-other-98        left                mostly true   \n",
       "1605  1605.xml  the-other-98        left                mostly true   \n",
       "1606  1606.xml  the-other-98        left                mostly true   \n",
       "1607  1607.xml  the-other-98        left                mostly true   \n",
       "1608  1608.xml  the-other-98        left                mostly true   \n",
       "1609  1609.xml  the-other-98        left                mostly true   \n",
       "1610  1610.xml  the-other-98        left                mostly true   \n",
       "1611  1611.xml  the-other-98        left                mostly true   \n",
       "1612  1612.xml  the-other-98        left                mostly true   \n",
       "1613  1613.xml  the-other-98        left  mixture of true and false   \n",
       "1614  1614.xml  the-other-98        left                mostly true   \n",
       "1615  1615.xml  the-other-98        left                mostly true   \n",
       "1616  1616.xml  the-other-98        left                mostly true   \n",
       "1617  1617.xml  the-other-98        left                mostly true   \n",
       "1618  1618.xml  the-other-98        left                mostly true   \n",
       "1619  1619.xml  the-other-98        left                mostly true   \n",
       "1620  1620.xml  the-other-98        left                mostly true   \n",
       "1621  1621.xml  the-other-98        left         no factual content   \n",
       "1622  1622.xml  the-other-98        left                mostly true   \n",
       "1623  1623.xml  the-other-98        left                mostly true   \n",
       "1624  1624.xml  the-other-98        left                mostly true   \n",
       "1625  1625.xml  the-other-98        left                mostly true   \n",
       "1626  1626.xml  the-other-98        left                mostly true   \n",
       "\n",
       "                                                    url  \n",
       "0     http://abcnews.go.com/Politics/impact-debates-...  \n",
       "1     http://abcnews.go.com/US/source-suspect-wanted...  \n",
       "2     http://abcnews.go.com/Politics/donald-trump-re...  \n",
       "3     http://abcnews.go.com/US/bombing-incidences-ny...  \n",
       "4     http://abcnews.go.com/Politics/trump-surrogate...  \n",
       "5     http://abcnews.go.com/International/woman-kill...  \n",
       "6     http://abcnews.go.com/International/syrian-mil...  \n",
       "7     http://abcnews.go.com/Politics/strait-talk-mat...  \n",
       "8     http://abcnews.go.com/Politics/ivanka-trump-se...  \n",
       "9     http://abcnews.go.com/Politics/obama-describes...  \n",
       "10    http://web.archive.org/web/20160920123248/http...  \n",
       "11    http://web.archive.org/web/20160923182935/http...  \n",
       "12    http://web.archive.org/web/20160920154511/http...  \n",
       "13    http://abcnews.go.com/Politics/terrorist-attac...  \n",
       "14    http://abcnews.go.com/Politics/ivanka-trump-me...  \n",
       "15    http://abcnews.go.com/Politics/hillary-clinton...  \n",
       "16    http://abcnews.go.com/Politics/presidential-de...  \n",
       "17    http://abcnews.go.com/International/russians-v...  \n",
       "18    http://web.archive.org/web/20160921200556/http...  \n",
       "19    http://abcnews.go.com/Politics/report-george-h...  \n",
       "20    http://abcnews.go.com/Politics/presidential-de...  \n",
       "21    http://abcnews.go.com/US/ny-nj-bombings-suspec...  \n",
       "22    http://abcnews.go.com/ABCNews/donald-trump-jr-...  \n",
       "23    http://web.archive.org/web/20160924113439/http...  \n",
       "24    http://abcnews.go.com/Politics/obama-recaps-pr...  \n",
       "25    http://abcnews.go.com/International/humanitari...  \n",
       "26    http://abcnews.go.com/Politics/donald-trump-jr...  \n",
       "27    http://abcnews.go.com/US/nyc-bombing-suspects-...  \n",
       "28    http://abcnews.go.com/US/fbi-opened-previous-i...  \n",
       "29    http://abcnews.go.com/Politics/hillary-clinton...  \n",
       "...                                                 ...  \n",
       "1597  http://usuncut.com/news/guess-which-terrorist-...  \n",
       "1598  http://usuncut.com/politics/robert-reich-how-t...  \n",
       "1599  http://usuncut.com/news/breaking-north-carolin...  \n",
       "1600  https://www.washingtonpost.com/news/worldviews...  \n",
       "1601  http://usuncut.com/black-lives-matter/gruesome...  \n",
       "1602  http://usuncut.com/news/louisiana-cop-caught-t...  \n",
       "1603  http://usuncut.com/class-war/elizabeth-warren-...  \n",
       "1604  http://usuncut.com/black-lives-matter/man-dies...  \n",
       "1605  http://usuncut.com/class-war/lunch-lady-quits-...  \n",
       "1606  http://grist.org/briefly/sweden-plans-to-give-...  \n",
       "1607  http://usuncut.com/black-lives-matter/veteran-...  \n",
       "1608  http://usuncut.com/black-lives-matter/ohio-pol...  \n",
       "1609  http://usuncut.com/black-lives-matter/can-slee...  \n",
       "1610  http://usuncut.com/news/trumps-african-america...  \n",
       "1611  http://usuncut.com/news/wells-fargo-fired-whis...  \n",
       "1612  http://usuncut.com/black-lives-matter/proteste...  \n",
       "1613    http://usuncut.com/climate/obama-two-pipelines/  \n",
       "1614  http://usuncut.com/news/officer-betty-shelby-c...  \n",
       "1615  http://usuncut.com/news/protester-shot-charlot...  \n",
       "1616  http://usuncut.com/black-lives-matter/video-sh...  \n",
       "1617  http://usuncut.com/world/here-are-9-reasons-de...  \n",
       "1618  http://usuncut.com/black-lives-matter/oprah-sp...  \n",
       "1619  http://usuncut.com/black-lives-matter/universi...  \n",
       "1620  http://usuncut.com/politics/bloomberg-fact-che...  \n",
       "1621  http://usuncut.com/politics/11-tweets-tonights...  \n",
       "1622  http://usuncut.com/news/houston-shooter-dresse...  \n",
       "1623  http://usuncut.com/politics/yes-donald-trump-s...  \n",
       "1624  http://usuncut.com/politics/clinton-crushes-tr...  \n",
       "1625  http://usuncut.com/black-lives-matter/cops-kil...  \n",
       "1626  http://usuncut.com/black-lives-matter/college-...  \n",
       "\n",
       "[1627 rows x 5 columns]>"
      ]
     },
     "execution_count": 2993,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_overview.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2994,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('BuzzData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2995,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Even if you are President Obama, your jokes ne...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Supporters of Republican presidential nominee ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>While presidential nominees Donald Trump and H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>A well known Black Lives Matter activist calle...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>A cafeteria worker in a Pennsylvania school di...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  prediction\n",
       "824   Even if you are President Obama, your jokes ne...           3\n",
       "355   Supporters of Republican presidential nominee ...           3\n",
       "315   While presidential nominees Donald Trump and H...           0\n",
       "517   A well known Black Lives Matter activist calle...           3\n",
       "1475  A cafeteria worker in a Pennsylvania school di...           3"
      ]
     },
     "execution_count": 2995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There have been so many deaths in country music over the past couple of months, and we thought that we were going to have to say goodbye to another legend Saturday night. It was a normal Saturday night at the Cornstock Concert on the Hill in Garnett, Kansas until Eddie Montgomery collapsed on stage in the middle of his performance. Many of you probably know the country star from the country duo Montgomery Gentry.  Montgomery Gentry was founded in 1999 when Eddie Montgomery and Troy Gentry came together and graced us all with their talents. The country duo has released 6 studio albums since their big debut and has had several No. 1 hits such as “If You Ever Stop Loving Me”, “Something to Be Proud Of”, “Lucky Man”, “Back When I Knew It All” and “Roll with Me”. The duo rose to fame shortly after the start of their career and they have continued to be a country favorite over the years. The duo was performing at their concert Saturday (September 24th) when Eddie Montgomery collapsed on the stage.  The medics immediately rushed to the stage and it was just moments later that he was walking off. The concert continued as Troy Gentry lead the band out. It was just four songs later that Eddie walked back out on the stage and apparently told the audience that he was not feeling well all day and that he was very dehydrated. And just like anything else, rumors started. Fans all over were saying that Eddie had a heart attack right there on stage. Apparently, they weren’t listening or they didn’t believe that he was actually dehydrated. The next morning on the duo’s Twitter they announced that Eddie was doing well and that it was definitely not a heart attack. There are false reports floating around that Eddie had a heart attack last night. NOT TRUE. He was dehydrated and returned to the stage. — Montgomery Gentry (@mgunderground) September 25, 2016  Eddie wanted to reassure everyone that he was indeed alive and that he didn’t have a heart attack, so a couple of hours after the initial post he took it upon himself to post a photo of him and his wife. Not only that, but he made sure that all of his fans knew that he was drinking PLENTY of water – doing his best to prevent the same thing from happening again. Eddie and Jennifer send their regards from Key West and thanks for all of the prayers and well wishes. And he's drinking plenty of H20! pic.twitter.com/xCJlTZCIHX — Montgomery Gentry (@mgunderground) September 25, 2016  We are so glad that Eddie is feeling better! I can’t imagine how scary this was for him and his fans!  We are praying for you Eddie!\""
      ]
     },
     "execution_count": 2996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.article[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mean of length article (in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2997,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data.article:\n",
    "    len_article = np.mean(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2998,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.0"
      ]
     },
     "execution_count": 2998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of article (by length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2999,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for length of article\n",
    "dist_article = []\n",
    "for x in data.article:\n",
    "    dist_article.append(len(x.split()))\n",
    "data['Article_length'] = dist_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.distplot(dist_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution by subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check for missing values\n",
    "- remove empty article\n",
    "- check for duplicated article\n",
    "- combine categories\n",
    "- remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article           0\n",
       "prediction        0\n",
       "Article_length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3002,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "                           article  prediction  Article_length\n",
      "94    The document has moved here.           3               5\n",
      "145   The document has moved here.           3               5\n",
      "308   The document has moved here.           1               5\n",
      "429   The document has moved here.           0               5\n",
      "724   The document has moved here.           3               5\n",
      "846   The document has moved here.           2               5\n",
      "908   The document has moved here.           3               5\n",
      "913   The document has moved here.           0               5\n",
      "1003  The document has moved here.           3               5\n",
      "1081  The document has moved here.           2               5\n",
      "1137  The document has moved here.           1               5\n",
      "1335  The document has moved here.           3               5\n",
      "1456  The document has moved here.           3               5\n",
      "1461  The document has moved here.           2               5\n"
     ]
    }
   ],
   "source": [
    "#find empty article \n",
    "empty_list = data[data['article'] == 'The document has moved here.']\n",
    "print(len(empty_list))\n",
    "print(empty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1590, 3)"
      ]
     },
     "execution_count": 3003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop empty articles\n",
    "data.drop(data[data['article'] == 'The document has moved here.'].index , inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "                                                article  prediction  \\\n",
      "223   (CNN)The circus of politics can often be a dis...           2   \n",
      "231   (CNN)Hillary Clinton forced Donald Trump onto ...           3   \n",
      "276   (CNN)Debate stages have long doubled as presid...           3   \n",
      "332   Washington (CNN)Republicans are preparing for ...           3   \n",
      "376   Washington (CNN)The battle over gun rights ret...           3   \n",
      "474   (CNN)Ukrainian President Petro Poroshenko says...           3   \n",
      "570   Washington (CNN)New York City Mayor Bill de Bl...           3   \n",
      "734   WashingtonSince we last updated the CNN battle...           3   \n",
      "771   (CNN)Sen. Jeff Sessions pilloried former Presi...           3   \n",
      "829   Washington (CNN)The cast of \"The West Wing\" is...           3   \n",
      "1023  (CNN)Even if their preferred candidate wins, a...           3   \n",
      "1128  (CNN)Hillary Clinton holds a solid 6-point lea...           3   \n",
      "1158  Washington (CNN)Hillary Clinton and Donald Tru...           3   \n",
      "1162  Washington (CNN)The cast of \"The West Wing\" is...           3   \n",
      "1209  Washington (CNN)The National Rifle Association...           3   \n",
      "1228  Washington (CNN)A series of high-profile breac...           3   \n",
      "1229  (CNN)Even if their preferred candidate wins, a...           3   \n",
      "1326  (CNN)Drawing enormous support from the state's...           3   \n",
      "1327  Washington (CNN)Rep. John Lewis, a pioneer in ...           3   \n",
      "1328  (CNN)Hillary Clinton forced Donald Trump onto ...           3   \n",
      "1330  (CNN)A group of 375 \"concerned\" scientists -- ...           3   \n",
      "1366  Washington (CNN)Senate Minority Leader Harry R...           3   \n",
      "1370  Washington (CNN)Former \"Miss Universe\" Alicia ...           3   \n",
      "1378  United Nations (CNN)President Barack Obama too...           3   \n",
      "1411  (CNN)Hillary Clinton's campaign manager said W...           3   \n",
      "1427  United Nations (CNN)President Barack Obama too...           3   \n",
      "1466  Washington (CNN)With two months to go before E...           3   \n",
      "1467  (CNN)President Barack Obama sharply criticized...           3   \n",
      "1515  Washington (CNN)The National Rifle Association...           3   \n",
      "1529  (CNN)Michelle Obama on Tuesday joked about the...           3   \n",
      "1531  New York (CNN)President Barack Obama confronte...           3   \n",
      "1532  WashingtonSince we last updated the CNN battle...           3   \n",
      "1581  (CNN)Former President Bill Clinton sought to a...           3   \n",
      "1587  (CNN)Former President George H.W. Bush said Mo...           0   \n",
      "\n",
      "      Article_length  \n",
      "223              602  \n",
      "231             1154  \n",
      "276              887  \n",
      "332             1285  \n",
      "376             1023  \n",
      "474              517  \n",
      "570              352  \n",
      "734              844  \n",
      "771              405  \n",
      "829              203  \n",
      "1023             305  \n",
      "1128             325  \n",
      "1158             296  \n",
      "1162             203  \n",
      "1209             360  \n",
      "1228            1050  \n",
      "1229             305  \n",
      "1326             254  \n",
      "1327             359  \n",
      "1328            1154  \n",
      "1330             337  \n",
      "1366             431  \n",
      "1370             797  \n",
      "1378            1114  \n",
      "1411             553  \n",
      "1427            1114  \n",
      "1466             777  \n",
      "1467             555  \n",
      "1515             360  \n",
      "1529             323  \n",
      "1531             881  \n",
      "1532             844  \n",
      "1581             356  \n",
      "1587             978  \n"
     ]
    }
   ],
   "source": [
    "# find dupilcated article\n",
    "dupl = data[data.duplicated(['article'])]\n",
    "print(len(dupl))\n",
    "print(dupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3005,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1557, 3)"
      ]
     },
     "execution_count": 3005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate\n",
    "data.drop_duplicates(keep='first', inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3006,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes = data.article.isin([\"EDITOR’S NOTE:\"]).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3007,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1210\n",
       "0     206\n",
       "1      80\n",
       "2      61\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 3008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine and drop categories of prediciton\n",
    "data.prediction.unique()\n",
    "data.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3009,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.prediction.drop(['2'])\n",
    "index_2 = data[data['prediction'] == 2].index\n",
    "data.drop(index_2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1496, 3)"
      ]
     },
     "execution_count": 3010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1210\n",
       "1     286\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 3011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine prediction of mostly & mixture of true and false since we are looking for fake news\n",
    "# -> dataset with prediction: 0 = mostly True news\n",
    "#                             1 = mostly Fake News & article with some fake news\n",
    "data.prediction.replace(0, 1, inplace = True)\n",
    "data.prediction.replace(3, 0, inplace = True)\n",
    "data.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers\n",
    "since we know that the distribution of length of words in the sample isn't Gaussian,\n",
    "we can't use the standard deviation of the sample as a cut-off \n",
    "therefore we use Interquartile Range Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th=307.000, 75th=599.500, IQR=292.500\n",
      "Identified outliers: 161\n",
      "Non-outlier observations: 1335\n"
     ]
    }
   ],
   "source": [
    "# calculate interquartile range\n",
    "q25, q75 = percentile(data.Article_length, 25), percentile(data.Article_length, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# identify outliers\n",
    "outliers = [idx for idx,x in data.Article_length.iteritems() if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data.Article_length if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3013,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 3013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of outliers\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3014,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "data.drop(outliers, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1335, 3)"
      ]
     },
     "execution_count": 3015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3016,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['article']\n",
    "y = data['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3017,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News and True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fig_age_survivial = (data.prediction.value_counts()\n",
    "                     .plot.pie(autopct='%1.1f%%'\n",
    "                               ,shadow=True\n",
    "                               ,colors=['blue','red']\n",
    "                               ,textprops=dict(color=\"w\",size='14'))\n",
    "                    )\n",
    "plt.title('What kind of news?')\n",
    "plt.legend(['True News','Fake News'])\n",
    "plt.savefig('figures/News_pie.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3019,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_article = []\n",
    "for x in data.article:\n",
    "    dist_article.append(len(x.split()))\n",
    "data['Article_length'] = dist_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3020,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distplot() got an unexpected keyword argument 'linestyle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3020-21446d8e1676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Article_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: distplot() got an unexpected keyword argument 'linestyle'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.distplot(data[\"Article_length\"], color=\"green\", label=\"A\", linestyle=\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot \n",
    "ax = sns.boxplot(x=data[\"Article_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot by prediction\n",
    "ax = sns.boxplot(x=\"prediction\", y=\"Article_length\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart for subject\n",
    "# boxplot for subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Article_length']\n",
    "text = data['article']\n",
    "label = data['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.article[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords (commonly used words which search engine has been programmed to ignore)\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing includes:\n",
    "- tokenize\n",
    "- remove punctuations and set lowercase\n",
    "- remove stopwords\n",
    "- remove URL's\n",
    "- remove twitter names\n",
    "- lemmatization / stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_lemm(text, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "    \n",
    "    ## remove URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    ## remove twitter\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "        \n",
    "    # \n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "data_lemm = data\n",
    "data_lemm[\"text\"] = data[\"article\"].apply(lambda x: \n",
    "          preprocess_text_lemm(x, flg_lemm=True, lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_lemm.text[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_stemm(text, flg_stemm=True, lst_stopwords=None):\n",
    "    \n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "    \n",
    "    ## remove URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    ## remove twitter\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "               \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "        \n",
    "    \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "data_stemm = data\n",
    "data_stemm[\"text\"] = data[\"article\"].apply(lambda x: \n",
    "          preprocess_text_stemm(x, flg_stemm=True, lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemm.drop(['article'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemm.text[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stemm.text[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data_lemm.head())\n",
    "print(data_stemm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data_lemm.shape)\n",
    "print(data_stemm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset for stemming data\n",
    "data_stemm_train, data_stemm_test = model_selection.train_test_split(data_stemm, test_size=0.3, random_state=RSEED)\n",
    "## get target\n",
    "y_stemm_train = data_stemm_train[\"prediction\"].values\n",
    "y_stemm_test = data_stemm_test[\"prediction\"].values\n",
    "## drop target\n",
    "X_stemm_train = data_stemm_train[\"prediction\"]\n",
    "X_stemm_test = data_stemm_test[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## split dataset for lemitaz data\n",
    "data_lemm_train, data_lemm_test = model_selection.train_test_split(data_lemm, test_size=0.3, random_state=RSEED)\n",
    "## get target\n",
    "y_lemm_train = data_lemm_train[\"prediction\"].values\n",
    "y_lemm_test = data_lemm_test[\"prediction\"].values\n",
    "## drop target\n",
    "X_lemm_train = data_lemm_train[\"prediction\"]\n",
    "X_lemm_test = data_lemm_test[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_stemm_test.shape)\n",
    "print(y_stemm_train.shape)\n",
    "print(X_lemm_test.shape)\n",
    "print(y_lemm_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for Convolutional Neural Network(CNN)\n",
    "#(X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn) = train_test_split(X_train, y_train, test_size=0.15, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud for data true news\n",
    "lst = ''\n",
    "for news in data_stemm[data_stemm.prediction==0].text.values:\n",
    "    lst += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 3000,\n",
    "    height = 2000,\n",
    "    background_color = 'black',\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(lst)\n",
    "fig = plt.figure(\n",
    "    figsize = (40, 30),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "del lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud for data fake news\n",
    "lst = ''\n",
    "for news in data_stemm[data_stemm.prediction==1].text.values:\n",
    "    lst += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 3000,\n",
    "    height = 2000,\n",
    "    background_color = 'black',\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(lst)\n",
    "fig = plt.figure(\n",
    "    figsize = (40, 30),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "del lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with Tf-Idf (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words model in general builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. Each word in the vocabulary becomes a feature and a document is represented by a vector with the same length of the vocabulary (a “bag of words”) \n",
    "\n",
    "But instead of simple counting, we use the term frequency–inverse document frequency (or Tf–Idf). Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.\n",
    "\n",
    "- Features Engineering with vectorizer\n",
    "- Features Selection by p-value\n",
    "- Model Design\n",
    "- Train / Test\n",
    "- Evaluate\n",
    "- Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2),lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering: create features by extracting information from the data\n",
    "corpus = data_stemm_train[\"text\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_stemm_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "X_stemm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_stemm_train.todense()[:,np.random.randint(0,data_stemm_train.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"trump\"\n",
    "dic_vocabulary[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection: in order to drop some columns and reduce the matrix dimensionality by selecting a subset of relevant variables:\n",
    "\n",
    "- perform a Chi-Square test to determine whether a feature and the target are independent\n",
    "\n",
    "- keep only the features with a certain p-value from the Chi-Square test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Perform feature selection using p-values (keep highly correlated features)\n",
    "def features_selection(X_stemm_train, y_stemm_train, X_names, top=None, print_top=10):    \n",
    "    ## selection\n",
    "    dtf_features = pd.DataFrame()\n",
    "    for cat in np.unique(y):\n",
    "        chi2, p = feature_selection.chi2(X, y==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame({\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>0.80] #p-value filter\n",
    "    if top is not None:\n",
    "        dtf_features = dtf_features.groupby('y')[\"y\",\"feature\",\"score\"].head(top)\n",
    "    \n",
    "    ## print\n",
    "    print(\"features selection: from\", \"{:,.0f}\".format(len(X_names)), \n",
    "          \"to\", \"{:,.0f}\".format(len(dtf_features[\"feature\"].unique())))\n",
    "    print(\" \")\n",
    "    for cat in np.unique(y):\n",
    "        print(\"# {}:\".format(cat))\n",
    "        print(\"  . selected features:\", len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "        print(\"  . top features:\", \", \".join(dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:print_top]))\n",
    "        print(\" \")\n",
    "    return dtf_features[\"feature\"].unique().tolist(), dtf_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#give features with p-value >0.80\n",
    "y = y_stemm_train\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.80\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_stemm_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\", len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "we will refit the vectorizer on the corpus by giving this new set of words as input\n",
    "-> produce a smaller feature matrix (8 features) and a shorter vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus)\n",
    "X_stemm_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dic_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.heatmap(X_stemm_train.todense()[:,np.random.randint(0,X_stemm_train.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def word_freq(corpus, ngrams=[1,2,3], top=10, figsize=(10,7)):\n",
    "    lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "    ngrams = [ngrams] if type(ngrams) is int else ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "dtf_freq = pd.DataFrame()\n",
    "for y in data[\"prediction\"].unique():\n",
    "    print(\"# {}:\".format(y))\n",
    "    dtf_y = word_freq(corpus=data_stemm_train[data_stemm_train[\"prediction\"]==y][\"text\"], ngrams=[1,2,3], top=10, figsize=(10,7))\n",
    "    print(dtf_y)\n",
    "    dtf_y[\"y\"] = y\n",
    "    dtf_freq = dtf_freq.append(dtf_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models using TFIDF Vectorizer¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(data_lemm, data_lemm['prediction'], test_size = 0.3, random_state= RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline for different algorithm\n",
    "def get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
    "    pipe = Pipeline([('vector', vectorizer),\n",
    "                    ('model', classifier)])\n",
    "    model = pipe.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix: \\n\", cm)\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# using three different classifiers\n",
    "classifiers = LogisticRegression(), ComplementNB(), DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for classifier in classifiers:\n",
    "    print(\"\\n\\n\", classifier)\n",
    "    print(\"***Usng TFIDF Vectorizer*** with stemmed data\")\n",
    "    get_prediction(vectorizer, classifier, X_stemm_train, X_stemm_test, y_stemm_train, y_stemm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model\n",
    "nlp = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set corpus\n",
    "corpus = data_stemm_train[\"text\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "               for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "\n",
    "## detect bigrams and trigrams\n",
    "bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                 delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "            delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit w2v\n",
    "nlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n",
    "            window=8, min_count=1, sg=1, iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select word\n",
    "word = \"trump\"\n",
    "nlp[word].shape\n",
    "fig = plt.figure()\n",
    "\n",
    "## word embedding\n",
    "tot_words = [word] + [tupla[0] for tupla in \n",
    "                 nlp.most_similar(word, topn=20)]\n",
    "X = nlp[tot_words]\n",
    "\n",
    "## pca to reduce dimensionality from 300 to 3\n",
    "pca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "## create dtf\n",
    "dtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\n",
    "dtf_[\"input\"] = 0\n",
    "dtf_[\"input\"].iloc[0:1] = 1\n",
    "\n",
    "## plot 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n",
    "           dtf_[dtf_[\"input\"]==0]['y'], \n",
    "           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n",
    "           dtf_[dtf_[\"input\"]==1]['y'], \n",
    "           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\n",
    "ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n",
    "       yticklabels=[], zticklabels=[])\n",
    "for label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n",
    "    x, y, z = row\n",
    "    ax.text(x, y, z, s=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform corpus into padded sequences of word ids to get a feature matrix.\n",
    "- Create an embedding matrix so that the vector of the word with id N is located at the Nth row.\n",
    "- Build a neural network with an embedding layer that weighs every word in the sequences with the   corresponding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "x_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(x_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look how it works\n",
    "\n",
    "i = 0\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(data_stemm_train[\"text\"].iloc[i].split())\n",
    "print(\"from: \", data_stemm_train[\"text\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(x_train[i])\n",
    "print(\"to: \", x_train[i], \"| len:\", len(x_train[i]))\n",
    "\n",
    "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
    "print(\"check: \", data_stemm_train[\"text\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[data_stemm_train[\"text\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for test data\n",
    "corpus = data_stemm_test['text']\n",
    "\n",
    "## create list of n-grams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
    "                 len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "    \n",
    "## detect common bigrams and trigrams using the fitted detectors\n",
    "lst_corpus = list(bigrams_detector[lst_corpus])\n",
    "lst_corpus = list(trigrams_detector[lst_corpus])\n",
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
    "\n",
    "## padding sequence\n",
    "x_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=15,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix of embedding that will be used as a weight matrix in the neural network classifier\n",
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"trump\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(15,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=15)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(3, activation='softmax')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and check the performance on a subset of the training set used for validation before testing it on the actual test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "## train\n",
    "training = model.fit(x=x_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show model \n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data\n",
    "predicted_prob = model.predict(x_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in \n",
    "             predicted_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_test))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_test = np.array([inverse_dic[y] for y in y_test])\n",
    "## train\n",
    "training = model.fit(x=x_test, y=y_test, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put an Attention layer in the neural network to extract the weights of each word and understand how much those contributed to classify an instance. So I’ll try to use Attention weights to build an explainer (similar to the one seen in the previous section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select observation\n",
    "i = 42\n",
    "txt_instance = data_stemm_test[\"text\"].iloc[i]\n",
    "## check true value and predicted value\n",
    "print(\"True:\", y_test[i], \"--> Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))\n",
    "\n",
    "## show explanation\n",
    "### 1. preprocess input\n",
    "lst_corpus = []\n",
    "for string in [re.sub(r'[^\\w\\s]','', txt_instance.lower().strip())]:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
    "                 len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "lst_corpus = list(bigrams_detector[lst_corpus])\n",
    "lst_corpus = list(trigrams_detector[lst_corpus])\n",
    "X_instance = kprocessing.sequence.pad_sequences(\n",
    "              tokenizer.texts_to_sequences(corpus), maxlen=15, \n",
    "              padding=\"post\", truncating=\"post\")\n",
    "### 2. get attention weights\n",
    "layer = [layer for layer in model.layers if \"attention\" in \n",
    "         layer.name][0]\n",
    "func = K.function([model.input], [layer.output])\n",
    "weights = func(X_instance)[0]\n",
    "weights = np.mean(weights, axis=2).flatten()\n",
    "### 3. rescale weights, remove null vector, map word-weight\n",
    "weights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)\n",
    "#weights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx \n",
    "#          != 0]\n",
    "dic_word_weigth = {word:weights[n] for n,word in \n",
    "                   enumerate(lst_corpus[0]) if word in \n",
    "                   tokenizer.word_index.keys()}\n",
    "### 4. barplot\n",
    "if len(dic_word_weigth) > 0:\n",
    "    dtf = pd.DataFrame.from_dict(dic_word_weigth, orient='index', \n",
    "                                columns=[\"score\"])\n",
    "    dtf.sort_values(by=\"score\", \n",
    "           ascending=True).tail(10).plot(kind=\"barh\", \n",
    "           legend=False).grid(axis='x')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"--- No word recognized ---\")\n",
    "### 5. produce html visualization\n",
    "text = []\n",
    "for word in lst_corpus[0]:\n",
    "    weight = dic_word_weigth.get(word)\n",
    "    if weight is not None:\n",
    "         text.append('<b><span style=\"background-color:rgba(100,149,237,' + str(weight) + ');\">' + word + '</span></b>')\n",
    "    else:\n",
    "         text.append(word)\n",
    "text = ' '.join(text)\n",
    "### 6. visualize on notebook\n",
    "print(\"\\033[1m\"+\"Text with highlighted words\")\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language model overcomes the biggest limitation of the classic Word Embedding approach: polysemy disambiguation (a word with different meanings) is identified by just one vector\n",
    "\n",
    "Google’s BERT (Bidirectional Encoder Representations from Transformers, 2018) combines ELMO context embedding and several Transformers, plus it’s bidirectional (which was a big novelty for Transformers). The vector BERT assigns to a word is a function of the entire sentence, therefore, a word can have different vectors based on the contexts\n",
    "\n",
    "For Fine-tuning the pre-trained model (transfer learning), I use a lighter version of BERT, called Distil-BERT (-> https://huggingface.co/transformers/model_doc/distilbert.html)\n",
    "\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert’s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "## bert model\n",
    "nlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distil-bert tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that I’m using the raw text as corpus (so far I’ve been using the clean_text column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data[\"text\"] #training data???\n",
    "maxlen = 50\n",
    "\n",
    "## add special tokens\n",
    "maxqnans = np.int((maxlen-20)/2)\n",
    "corpus_tokenized = [\"[CLS] \"+\n",
    "             \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "             str(txt).lower().strip()))[:maxqnans])+\n",
    "             \" [SEP] \" for txt in corpus]\n",
    "\n",
    "## generate masks\n",
    "masks = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(\n",
    "           txt.split(\" \"))) for txt in corpus_tokenized]\n",
    "    \n",
    "## padding\n",
    "txt2seq = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in corpus_tokenized]\n",
    "    \n",
    "## generate idx\n",
    "idx = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq]\n",
    "    \n",
    "## generate segments\n",
    "segments = [] \n",
    "for seq in txt2seq:\n",
    "    temp, i = [], 0\n",
    "    for token in seq.split(\" \"):\n",
    "        temp.append(i)\n",
    "        if token == \"[SEP]\":\n",
    "             i += 1\n",
    "    segments.append(temp)\n",
    "## feature matrix\n",
    "X_train = [np.asarray(idx, dtype='int32'), \n",
    "           np.asarray(masks, dtype='int32'), \n",
    "           np.asarray(segments, dtype='int32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"txt: \", data[\"text\"].iloc[0])\n",
    "print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "print(\"idx: \", X_train[0][i])\n",
    "print(\"mask: \", X_train[1][i])\n",
    "print(\"segment: \", X_train[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for test data\n",
    "corpus = data[\"text\"] ### TEST DATA!!!\n",
    "maxlen = 50\n",
    "\n",
    "## add special tokens\n",
    "maxqnans = np.int((maxlen-20)/2)\n",
    "corpus_tokenized = [\"[CLS] \"+\n",
    "             \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "             str(txt).lower().strip()))[:maxqnans])+\n",
    "             \" [SEP] \" for txt in corpus]\n",
    "\n",
    "## generate masks\n",
    "masks = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(\n",
    "           txt.split(\" \"))) for txt in corpus_tokenized]\n",
    "    \n",
    "## padding\n",
    "txt2seq = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in corpus_tokenized]\n",
    "    \n",
    "## generate idx\n",
    "idx = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq]\n",
    "    \n",
    "## generate segments\n",
    "segments = [] \n",
    "for seq in txt2seq:\n",
    "    temp, i = [], 0\n",
    "    for token in seq.split(\" \"):\n",
    "        temp.append(i)\n",
    "        if token == \"[SEP]\":\n",
    "             i += 1\n",
    "    segments.append(temp)\n",
    "## feature matrix\n",
    "X_train = [np.asarray(idx, dtype='int32'), \n",
    "           np.asarray(masks, dtype='int32'), \n",
    "           np.asarray(segments, dtype='int32')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model with transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summarize the output of BERT into one vector with Average Pooling \n",
    "- Add two final Dense layers to predict the probability of each news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs\n",
    "idx = layers.Input((50), dtype=\"int32\", name=\"input_idx\")\n",
    "masks = layers.Input((50), dtype=\"int32\", name=\"input_masks\")\n",
    "## pre-trained bert with config\n",
    "config = transformers.DistilBertConfig(dropout=0.2, \n",
    "           attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "bert_out = nlp(idx, attention_mask=masks)[0]\n",
    "## fine-tuning\n",
    "x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "y_out = layers.Dense(len(np.unique(y_train)), \n",
    "                     activation='softmax')(x)\n",
    "## compile\n",
    "model = models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "## train\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=64, \n",
    "                     epochs=1, shuffle=True, verbose=1, \n",
    "                     validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in \n",
    "             predicted_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
